{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from calendar import timegm\n",
    "\n",
    "def strf(s):\n",
    "    return time.strftime('%Y-%m-%d %H:%M', time.gmtime(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708845e2-7246-434c-acee-c02cbc8861c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_pages(method, params, max_pages=1000):\n",
    "    data = []\n",
    "    has_more = True\n",
    "    page = 1\n",
    "    while has_more and page<=max_pages:\n",
    "        params['page'] = page\n",
    "        try:\n",
    "            resp = requests.get('https://api.stackexchange.com/2.3/' + method, params=params).json()\n",
    "            page+=1\n",
    "            data += resp['items']\n",
    "            has_more = resp['has_more']\n",
    "        except Exception as e:\n",
    "            raise Exception(f'last response: {resp}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ee7f75-e6ec-4535-88f0-f67285b4563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_related(acc_token, key, qid, \n",
    "                  from_date : str | int = None, \n",
    "                  to_date : str | int = None,\n",
    "                 ):\n",
    "    params = {\n",
    "        'site': 'stats',\n",
    "        'pagesize': 100,\n",
    "        'sort': 'creation',\n",
    "        'order': 'asc',\n",
    "        'key': key,\n",
    "        'access_token': acc_token,\n",
    "       # 'filter': '!nNPvSNPI7A', # include body\n",
    "    }\n",
    "    for arg,key in zip([from_date, to_date], ['fromdate', 'todate']):\n",
    "        if arg is not None:\n",
    "            if isinstance(arg, int):\n",
    "                params[key] = arg\n",
    "            elif isinstance(arg, str):\n",
    "                params[key] = int(timegm(time.strptime(arg, '%Y-%m-%d %H:%M')))\n",
    "            else:\n",
    "                raise ValueError(f'unsupported type `{arg}`: {type(arg)}')\n",
    "        \n",
    "    method = f\"questions/{qid}/related\"\n",
    "    data = fetch_all_pages(method, params)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222af2d-2e89-41a3-9290-eb690670f463",
   "metadata": {},
   "source": [
    "### API Retrieval\n",
    "\n",
    "First retrieve questions, then their related questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23109e86-f203-4489-a833-34005d0ed656",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_token = '##########'\n",
    "key = '########'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d14fb-8dfb-4e4b-bf31-8ea0658b5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'site': 'stats',\n",
    "        'pagesize': 100,\n",
    "        'sort': 'creation',\n",
    "        'order': 'asc',\n",
    "        'key': key,\n",
    "        'access_token': acc_token,\n",
    "        'fromdate': int(timegm(time.strptime('2014-01-01 00:00', '%Y-%m-%d %H:%M'))),\n",
    "        'todate': int(timegm(time.strptime('2015-01-01 00:00', '%Y-%m-%d %H:%M'))),    \n",
    "        #'filter': '!nNPvSNPI7A',\n",
    "}\n",
    "\n",
    "method = 'questions'\n",
    "questions = fetch_all_pages(method, params)\n",
    "#resp = requests.get('https://api.stackexchange.com/2.3/' + method, params=params)#.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "related = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e34887-b8aa-4e99-8460-30dccd0ba97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = None\n",
    "for q in tqdm(questions[len(related):]):\n",
    "    try:\n",
    "        data = fetch_related(acc_token, key, q['Id'], to_date=int(q['uxtime']))\n",
    "        related[q['Id']] = data\n",
    "    except Exception as err:\n",
    "        time.sleep(3)\n",
    "        e = err\n",
    "        if not isinstance(e, requests.exceptions.ConnectTimeout):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff16935-c328-403f-a2b3-d2fc4e67d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('related_15835_2014-01-01_2014-12-31.json', 'w') as f:\n",
    "    json.dump(related, f)\n",
    "\n",
    "with open('questions_15835_2014-01-01_2014-12-31_related.json', 'w') as f:\n",
    "    json.dump(related, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd3bf6-2415-4df8-a707-e1a8e834aca3",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "\n",
    "Note: 404 pages result when you access deleted questions. Those pages contain a list of related questions too (related to the original one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0'\n",
    "}\n",
    "headers2 = { 'User-Agent': 'bot 1.1' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa17dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = []\n",
    "redirected = []\n",
    "redirect_attempts = defaultdict(int)\n",
    "ids404 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3af27e-b5e8-4a81-a17e-d6539322b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "exc = None\n",
    "with tqdm(total=len(ids)-start_ix) as pbar:\n",
    "    while i<len(ids):\n",
    "        try:\n",
    "            status = 429\n",
    "            while status==429:\n",
    "                resp = requests.get(f'https://stats.stackexchange.com/questions/{ids[i]}',\n",
    "                            headers=headers)\n",
    "                status = resp.status_code\n",
    "                if status==429:\n",
    "                    if redirect_attempts[ids[i]]==10:\n",
    "                        break\n",
    "                    redirect_attempts[ids[i]]+=1\n",
    "                    time.sleep(10)\n",
    "            \n",
    "        except requests.exceptions.TooManyRedirects as e:\n",
    "            redirected.append(ids[i])\n",
    "            i+=1\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            exc = e\n",
    "            resps.append(resp)\n",
    "            raise\n",
    "        soup = BeautifulSoup(resp.content, 'lxml')\n",
    "        \n",
    "        if status==404:\n",
    "            ids404[ids[i]] = resp\n",
    "            links = soup.find_all('a', {'class': 'question-not-found'})\n",
    "            related[ids[i]] = [ int(l['href'].split('/')[2]) for l in links]\n",
    "        elif status==200:    \n",
    "            rel = soup.find_all('div', {'class':'related js-gps-related-questions'})\n",
    "            if len(rel)==0:\n",
    "                related[ids[i]] = []\n",
    "            elif len(rel)==1:\n",
    "                links = rel[0].find_all('a', {'class': 'question-hyperlink'})\n",
    "                related[ids[i]] = [ int(l['href'].split('/')[2]) for l in links]\n",
    "            else:\n",
    "                raise ValueError(f'rel sz: {len(rel)}, id: {ids[i]}')\n",
    "            \n",
    "            #lin = soup.find_all('div', {'class':'linked'})\n",
    "            #if len(lin)==0:\n",
    "            #    linked[ids[i]] = []\n",
    "            #elif len(lin)==1:\n",
    "            #    links = lin[0].find_all('a', {'class':'question-hyperlink'})\n",
    "            #    linked[ids[i]] = [ int(l['href'].split('/')[2]) for l in links]\n",
    "            #else:\n",
    "            #    raise ValueError(f'linked sz: {len(lin)}, id: {ids[i]}')\n",
    "        elif status==429:\n",
    "            pass\n",
    "        else:\n",
    "            resps.append(resp)\n",
    "            raise ValueError(f'weird status: {status}')\n",
    "        i+=1\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bb3aa-1fd9-4f4f-8d95-f07518252949",
   "metadata": {},
   "source": [
    "Reputation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9ee99-f12d-4f47-b3bd-ad828b881716",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(ix, len(ids)//100), total=len(ids)//100-ix):\n",
    "    id_batch = ';'.join([x for x in ids[i*100 : (i+1)*100]])\n",
    "    method = f'users/{id_batch}/reputation-history'\n",
    "    hist += fetch_all_pages(method, params)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d6978-dec8-4f18-812a-e5b22448d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reputation_hist_5.json', 'w') as f:\n",
    "    json.dump({'items': hist}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Docs GPT (py 3.10)",
   "language": "python",
   "name": "docs_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
